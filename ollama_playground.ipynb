{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634f78e0",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Hello, Ollama!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8afde13",
   "metadata": {},
   "source": [
    "This notebook is meant to be a tiny lab for experimenting with local LLMs via Ollama.\n",
    "\n",
    "What we do here:\n",
    "\n",
    "- Pull a small local model with Ollama\n",
    "- Send a simple one-shot prompt from Python\n",
    "- Try **streaming** responses\n",
    "- Build a minimal **chat loop** that keeps history\n",
    "- Add **JSON logging** so each chat session is saved for later analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1c60e",
   "metadata": {},
   "source": [
    "### 1. Load a local model with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab1bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067a8f3",
   "metadata": {},
   "source": [
    "First, we pick a model name and aks Ollama to pull it locally. If you already have it, this is basically a no-op.\n",
    "\n",
    "You can swap `phi3` fro any other model you've installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c45c2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"phi3\" # will try this one first then maybe \"qwen:3b\"\n",
    "ollama.pull(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c8f894",
   "metadata": {},
   "source": [
    "### 2. Simple helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d6d9c1",
   "metadata": {},
   "source": [
    "This helper:\n",
    "\n",
    "- Sends a **single** user message.\n",
    "- Uses a fixed `temperature` and `num_predict` (max new tokens).\n",
    "- Returns the full response as a string (non-streaming: we wait until it’s done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc09cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str) -> str:\n",
    "    response = ollama.chat(\n",
    "        model = MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\n",
    "            \"temperature\": 0.7,\n",
    "            \"num_predict\": 512,\n",
    "        }\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c43198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an AI and I don't have feelings, but thank you for asking! How can I assist you on this fine day? If there is anything specific that concerns or interests you, please let me know. Whether it's information, problem-solving, creativity sparking questions – just shoot away!\n"
     ]
    }
   ],
   "source": [
    "print(generate_response(\"Hello Ollama, how are you today?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2569b41",
   "metadata": {},
   "source": [
    "### 3. Let's try streaming!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d2b9f",
   "metadata": {},
   "source": [
    "Now we switch to **streaming mode**:\n",
    "\n",
    "- Instead of waiting for the full answer, we iterate over chunks.\n",
    "- Each chunk contains part of the text, so we can print it as it arrives.\n",
    "- This is how modern chat UIs feel fast and “alive”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3873025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_streaming(prompt: str):\n",
    "    stream = ollama.chat(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }],\n",
    "        stream=True,\n",
    "        options={\n",
    "            \"temparature\": 0.7,\n",
    "            \"num_predict\": 256\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    full = []\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        print(content, end=\"\", flush=True)\n",
    "        full.append(content)\n",
    "    print()\n",
    "    return \"\".join(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ad03d",
   "metadata": {},
   "source": [
    "If you run the cell below, you’ll see the answer appear **token by token**,\n",
    "instead of all at once at the end.\n",
    "\n",
    "This is important for UX: even if the model takes a while to finish,\n",
    "the user sees progress immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66aeeca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to us due to a phenomenon called Rayleigh scattering. When sunlight enters Earth' end, it encounters molecules and small particles in our atmosphere which cause the light to scatter. Blue light scatters more because its shorter wavelength is better suited for interaction with these tiny atmospheric components than longer-wavelength red or yellow light. So when we look up at the sky away from the sun, it’s dominated by this scattered blue light that reaches our eyes, making the sky appear predominantly blue during daylight hours.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The sky appears blue to us due to a phenomenon called Rayleigh scattering. When sunlight enters Earth' end, it encounters molecules and small particles in our atmosphere which cause the light to scatter. Blue light scatters more because its shorter wavelength is better suited for interaction with these tiny atmospheric components than longer-wavelength red or yellow light. So when we look up at the sky away from the sun, it’s dominated by this scattered blue light that reaches our eyes, making the sky appear predominantly blue during daylight hours.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_streaming('Why is the sky blue?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915d3a4",
   "metadata": {},
   "source": [
    "### 4. Let's try simple chat loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a35dced",
   "metadata": {},
   "source": [
    "Next, we build a tiny CLI chat loop:\n",
    "\n",
    "- Keeps a `history` list of `{role, content}` messages.\n",
    "- Sends the **full history** on each request, so the model has context.\n",
    "- Streams tokens as they come.\n",
    "- Type `exit`, `\\q` or `\\quit` to end the session.\n",
    "\n",
    "This is the first baby step toward a *real* local agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1fd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat with phi3. Type 'exit' to quit.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "MODEL_NAME = \"phi3\"  # You can change to another model if you have it pulled locally\n",
    "\n",
    "\n",
    "def chat_loop(model=MODEL_NAME, temperature=1.1, num_predict=256):\n",
    "    history = []\n",
    "    print(f\"Starting chat with {model}. Type 'exit' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"> \")\n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"\\nending chat...\")\n",
    "            break\n",
    "\n",
    "        if user_input.strip().lower() in {\"exit\", \"\\\\q\", \"\\\\quit\"}:\n",
    "            print(\"ending chat...\")\n",
    "            break\n",
    "\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        stream = ollama.chat(\n",
    "            model=model,\n",
    "            messages=history,\n",
    "            options={\n",
    "                \"temperature\": temperature,  # try more randomness\n",
    "                \"num_predict\": num_predict,\n",
    "            },\n",
    "            stream=True,\n",
    "        )\n",
    "        full_chunks = []\n",
    "        print(f\"{model}: \", end=\"\")\n",
    "        for chunk in stream:\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_chunks.append(content)\n",
    "        print()\n",
    "\n",
    "        assistant_message = \"\".join(full_chunks)\n",
    "        history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6206a",
   "metadata": {},
   "source": [
    "You can also try this loop from a standalone script\n",
    "(e.g. `chat_loop.py`) so it feels more like a tiny local “terminal chat app”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d46face",
   "metadata": {},
   "source": [
    "### 5. Logging chat history to JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e6108",
   "metadata": {},
   "source": [
    "Right now, `history` only lives in RAM. Once the process exits, the whole\n",
    "conversation is gone.\n",
    "\n",
    "To take a small step toward an **agent brain**, we’ll:\n",
    "\n",
    "- Log each chat session to a **single JSON file**.\n",
    "- Use a simple schema with:\n",
    "  - `model`, `temperature`, `num_predict`\n",
    "  - `started_at`, `ended_at`\n",
    "  - `messages`: list of `{role, content, timestamp}`\n",
    "\n",
    "This is enough to:\n",
    "\n",
    "- Re-read conversations later.\n",
    "- Use logs as evaluation data.\n",
    "- Feed logs into future RAG / analytics tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import ollama\n",
    "\n",
    "MODEL_NAME = \"gemma3:1b\"\n",
    "\n",
    "def chat_loop(model: str = MODEL_NAME, temperature: float = 1.1, num_predict: int = 512):\n",
    "    history = []\n",
    "    print(f\"Starting chat with {model}. Type '\\\\exit', '\\\\q' or '\\\\quit' to quit.\")\n",
    "    \n",
    "    session_start = datetime.now().isoformat\n",
    "    log = {\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"num_predict\": num_predict,\n",
    "        \"started_at\": session_start,\n",
    "        \"messages\": [],\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"> \")\n",
    "            except (KeyboardInterrupt, EOFError):\n",
    "                print(\"\\nerror, ending chat...\")\n",
    "                break\n",
    "            \n",
    "            if user_input.strip().lower() in {\"\\\\exit\", \"\\\\q\", \"\\\\quit\"}:\n",
    "                print(\"\\n\\quit command, ending chat...\")\n",
    "                break\n",
    "            \n",
    "            now = datetime.now().isoformat()\n",
    "            \n",
    "            user_msg = {\"role\": \"user\", \"content\": user_input}\n",
    "            history.append(user_msg)\n",
    "            log[\"messages\"].append(\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_input,\n",
    "                    \"timestamp\": now,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            stream = ollama.chat(\n",
    "                model=model,\n",
    "                messages=history,\n",
    "                options={\n",
    "                    \"temperature\": temperature,  # try more randomness\n",
    "                    \"num_predict\": num_predict,\n",
    "                },\n",
    "                stream=True,\n",
    "            )\n",
    "            full_chunks = []\n",
    "            print(f\"{model}: \", end=\"\")\n",
    "            for chunk in stream:\n",
    "                content = chunk[\"message\"][\"content\"]\n",
    "                print(content, end=\"\", flush=True)\n",
    "                full_chunks.append(content)\n",
    "            print()\n",
    "            \n",
    "            assistant_message = \"\".join(full_chunks)\n",
    "            assistant_msg = {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "            history.append(assistant_msg)\n",
    "            \n",
    "            now = datetime.now().isoformat()\n",
    "            \n",
    "            log[\"messages\"].append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": assistant_message,\n",
    "                    \"timestamp\": now,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "    finally:\n",
    "        log[\"ended_at\"] = datetime.now().isoformat()\n",
    "        os.makedirs(\"chat_logs\", exist_ok=True)\n",
    "        \n",
    "        safe_start = session_start.replace(\":\", \"-\")\n",
    "        log_path = os.path.join(\"chat_logs\", f\"chat_{safe_start}.json\")\n",
    "        \n",
    "        with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(log, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        print(f\"chat log saved to {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918c8c3",
   "metadata": {},
   "source": [
    "### 6. Using the loggin loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff43be93",
   "metadata": {},
   "source": [
    "To try this logging variant:\n",
    "\n",
    "1. Run `chat_loop_with_logging()` in this notebook **or** from a script.\n",
    "2. Ask a few questions.\n",
    "3. Exit with `\\exit`, `\\q`, `\\quit` or Ctrl+C.\n",
    "4. Check the `chat_logs/` directory – you should see a file like:\n",
    "\n",
    "   ```text\n",
    "   chat_logs/chat_2025-11-30T16-25-12.345678.json\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
