{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634f78e0",
   "metadata": {},
   "source": [
    "## This is playground repo to play around with local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab1bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c45c2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"phi3\" # will try this one first then maybe \"qwen:3b\"\n",
    "ollama.pull(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc09cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str) -> str:\n",
    "    response = ollama.chat(\n",
    "        model = MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\n",
    "            \"temperature\": 0.7,\n",
    "            \"num_predict\": 512,\n",
    "        }\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c43198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a string of code here with no feelings or physical form, so I don't experience days the way humans do. However, if I were to simulate an answer: \"I'm functioning optimally and ready for our interaction! How can I help you today?\"\n"
     ]
    }
   ],
   "source": [
    "print(generate_response(\"Hello Ollama, how are you today?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2569b41",
   "metadata": {},
   "source": [
    "### Let's try streaming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3873025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_streaming(prompt: str):\n",
    "    stream = ollama.chat(\n",
    "        model = MODEL_NAME,\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }],\n",
    "        stream=True,\n",
    "        options={\n",
    "            \"temparature\": 0.7,\n",
    "            \"num_predict\": 256\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    full = []\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        print(content, end=\"\", flush=True)\n",
    "        full.append(content)\n",
    "    print()\n",
    "    return \"\".join(full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ad03d",
   "metadata": {},
   "source": [
    "If you run it now, you can see how tokens are returning as they are generated, not all at once in the end, this is important feature for UX, and most of chat interfaces use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66aeeca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The color of the sky appears predominantly blue due to a phenomenon called Rayleigh scattering. This effect occurs when molecules and small particles in Earth's atmosphere interact with sunlight, causing shorter wavelength light (blue) to scatter more than longer wavelength light (red). Since our eyes are most sensitive to the green-yellow part of the spectrum, we perceive that color as being \"whitish\" or blue.\n",
      "\n",
      "When there are fewer particles in the atmosphere, such as on clear sunny days with little pollution and moisture content—like during a dry day without rain clouds present —the sky appears even bluer because less light is absorbed before it reaches our eyes. However, when there'selavent occurs more atmospheric particulates like dust or water droplets from cloud formation due to humidity increases the scattering and reflection of sunlight in all directions which can lead to a whiter sky with possible pinkish undertones during events such as halos around the sun.\n",
      "\n",
      "During twilight periods, when some direct light reaches Earth's surface while also reflecting off clouds or atmospheric particles (known as Mie scattering), we may observe sh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The color of the sky appears predominantly blue due to a phenomenon called Rayleigh scattering. This effect occurs when molecules and small particles in Earth\\'s atmosphere interact with sunlight, causing shorter wavelength light (blue) to scatter more than longer wavelength light (red). Since our eyes are most sensitive to the green-yellow part of the spectrum, we perceive that color as being \"whitish\" or blue.\\n\\nWhen there are fewer particles in the atmosphere, such as on clear sunny days with little pollution and moisture content—like during a dry day without rain clouds present —the sky appears even bluer because less light is absorbed before it reaches our eyes. However, when there\\'selavent occurs more atmospheric particulates like dust or water droplets from cloud formation due to humidity increases the scattering and reflection of sunlight in all directions which can lead to a whiter sky with possible pinkish undertones during events such as halos around the sun.\\n\\nDuring twilight periods, when some direct light reaches Earth\\'s surface while also reflecting off clouds or atmospheric particles (known as Mie scattering), we may observe sh'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_streaming('Why sky is blue?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5915d3a4",
   "metadata": {},
   "source": [
    "### Let's try simple chat loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1fd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting chat with phi3. Type 'exit' to quit.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "MODEL_NAME = \"phi3\"  # You can change to another model if you have it pulled locally\n",
    "\n",
    "\n",
    "def chat_loop(model=MODEL_NAME, temperature=1.1, num_predict=256):\n",
    "    history = []\n",
    "    print(f\"Starting chat with {model}. Type 'exit' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"> \")\n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"\\nending chat...\")\n",
    "            break\n",
    "\n",
    "        if user_input.strip().lower() in {\"exit\", \"\\\\q\", \"\\\\quit\"}:\n",
    "            print(\"ending chat...\")\n",
    "            break\n",
    "\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        stream = ollama.chat(\n",
    "            model=model,\n",
    "            messages=history,\n",
    "            options={\n",
    "                \"temperature\": temperature,  # try more randomness\n",
    "                \"num_predict\": num_predict,\n",
    "            },\n",
    "            stream=True,\n",
    "        )\n",
    "        full_chunks = []\n",
    "        print(f\"{model}: \", end=\"\")\n",
    "        for chunk in stream:\n",
    "            content = chunk[\"message\"][\"content\"]\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full_chunks.append(content)\n",
    "        print()\n",
    "\n",
    "        assistant_message = \"\".join(full_chunks)\n",
    "        history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6206a",
   "metadata": {},
   "source": [
    "You should try this loop implemented in chat_loop.py\n",
    "\n",
    "Now let's try to add one more thing to our simple loop.. For now we are saving `history` list in RAM. It works, but it doens't work very good. So let's do one more baby step toward building an agent brain. We will log our chat history and put it in appropriate format (JSON) with timestamps. For this exercise we going to use one JSON file per chat session.\n",
    "\n",
    "What we will log:\n",
    "- model, temperature, num_predict\n",
    "- started_at, ended_at\n",
    "- messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import ollama\n",
    "\n",
    "MODEL_NAME = \"gemma3:1b\"\n",
    "\n",
    "def chat_loop(model: str = MODEL_NAME, temperature: float = 1.1, num_predict: int = 512):\n",
    "    history = []\n",
    "    print(f\"Starting chat with {model}. Type '\\\\exit', '\\\\q' or '\\\\quit' to quit.\")\n",
    "    \n",
    "    session_start = datetime.now().isoformat\n",
    "    log = {\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"num_predict\": num_predict,\n",
    "        \"started_at\": session_start,\n",
    "        \"messages\": [],\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"> \")\n",
    "            except (KeyboardInterrupt, EOFError):\n",
    "                print(\"\\nerror, ending chat...\")\n",
    "                break\n",
    "            \n",
    "            if user_input.strip().lower() in {\"\\\\exit\", \"\\\\q\", \"\\\\quit\"}:\n",
    "                print(\"\\n\\quit command, ending chat...\")\n",
    "                break\n",
    "            \n",
    "            now = datetime.now().isoformat()\n",
    "            \n",
    "            user_msg = {\"role\": \"user\", \"content\": user_input}\n",
    "            history.append(user_msg)\n",
    "            log[\"messages\"].append(\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_input,\n",
    "                    \"timestamp\": now,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            stream = ollama.chat(\n",
    "                model=model,\n",
    "                messages=history,\n",
    "                options={\n",
    "                    \"temperature\": temperature,  # try more randomness\n",
    "                    \"num_predict\": num_predict,\n",
    "                },\n",
    "                stream=True,\n",
    "            )\n",
    "            full_chunks = []\n",
    "            print(f\"{model}: \", end=\"\")\n",
    "            for chunk in stream:\n",
    "                content = chunk[\"message\"][\"content\"]\n",
    "                print(content, end=\"\", flush=True)\n",
    "                full_chunks.append(content)\n",
    "            print()\n",
    "            \n",
    "            assistant_message = \"\".join(full_chunks)\n",
    "            assistant_msg = {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "            history.append(assistant_msg)\n",
    "            \n",
    "            now = datetime.now().isoformat()\n",
    "            \n",
    "            log[\"messages\"].append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": assistant_message,\n",
    "                    \"timestamp\": now,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "    finally:\n",
    "        log[\"ended_at\"] = datetime.now().isoformat()\n",
    "        os.makedirs(\"chat_logs\", exist_ok=True)\n",
    "        \n",
    "        safe_start = session_start.replace(\":\", \"-\")\n",
    "        log_path = os.path.join(\"chat_logs\", f\"chat_{safe_start}.json\")\n",
    "        \n",
    "        with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(log, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        print(f\"chat log saved to {log_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918c8c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff43be93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
